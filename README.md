# GPT2-Exploration

This project implements a custom GPT-2-style language model pipeline from scratch using PyTorch. It features full training, evaluation, logging, distributed training support, and token-level streaming from disk. The setup is modular and research-friendly, designed for experimentation with scaling and architecture changes.

---

## âœ… Features Overview

### ğŸ§  Model
- GPT-2-style Transformer decoder blocks
- Token and position embeddings (`wte`, `wpe`)
- Configurable architecture via `GPTConfig`
- `torch.compile` integration for optimization

### ğŸ› ï¸ Training
- Cosine LR decay with linear warmup
- Mixed-precision training (`torch.amp`)
- Optional gradient accumulation
- DDP-compatible for multi-GPU training
- Gradient clipping and norm tracking

### ğŸ“¦ Data Pipeline
- Stream large tokenized datasets from disk
- Custom `DirStreamingDataset` (no memory blowup)
- Deterministic train/val split via seed

### ğŸ“‰ Logging & Evaluation
- Sample generations printed at regular intervals
- Periodic val loss computation
- Track TPS, gradient norms, LR, and loss
- Outputs: `.pth` model, `.json` stats, `.npy` loss array, `.png` loss curve

---

## ğŸ› ï¸ Setup

Install dependencies with:
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Usage

### Training
```bash
python gpt2.py --mode train
```

### Evaluation / Inference
```bash
python gpt2.py --mode eval
```

---

## ğŸ”¢ Configurations

- `B`, `T` = Microbatch size and sequence length
- `batch_size` = Total tokens per step (via grad accumulation)
- `max_steps` = Total training steps
- `max_lr` = Peak learning rate

These are auto-handled:
```python
grad_accum_steps = batch_size // (B * T)
```

---

## ğŸ—ï¸ Structure

```
GPT2-exploration/
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ openwebtext/ 
â”‚   â”‚   â”œâ”€â”€ tokens_part_x.pt
â”‚   â”œâ”€â”€ smol-smoltalk/
â”‚   â”‚   â”œâ”€â”€ tokens_part_x.pt
â”‚   â”œâ”€â”€ openwebtext.py #run this to create openwebtext token DIR
â”‚   â”œâ”€â”€ openwebtext.py #run this to create smol-smoltalk token DIR
â”œâ”€â”€ GPT2-simple/
â”‚   â”œâ”€â”€ gpt2.py
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ GPT2_builder.py
â”‚   â”‚   â””â”€â”€ model_tracker.py
â”‚   â”œâ”€â”€ dataloaders/
â”‚   â”‚   â”œâ”€â”€ dataloader_lite.py (DEPRECIATED)
â”‚   â”‚   â”œâ”€â”€ token_text_dataset.py (DEPRECIATED)
â”‚   â”‚   â””â”€â”€ dir_streaming_dataset.py
â”‚   â”œâ”€â”€ hellaswag/
â”‚   â”‚   â””â”€â”€ hellaswag_val.jsonl
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ hellaswag.py       # Hellaswag Eval
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ gpt-2/
â”‚   â”‚   â””â”€â”€ gpt-2.pth
â”œâ”€â”€ analytics/
â”‚   â””â”€â”€ gpt-2/
â”‚       â”œâ”€â”€ plots/         # Loss curves
â”‚       â””â”€â”€ stats/         # Logs and losses
â”œâ”€â”€ requirements.txt
```

---

## ğŸ§­ Roadmap

- [ ] Scale Model Architechture
- [ ] Add FlashAttention 2 support
- [ ] Switch to SwiGLU + RMSNorm
- [ ] Incorporate Rotary Positional Embeddings
- [ ] ONNX export and HF compatibility
- [ ] Multiple dataset mixing + shuffling
- [ ] More Analytics!!! Validation Accuracy plotting

---

## âš ï¸ Notes
- For DDP, use `torchrun` or `accelerate` to launch across GPUs
- When using `DistributedDataParallel`, access model as `model.module`
- Ensure vocab size and sequence length match when resuming from checkpoint
- Supports graceful `KeyboardInterrupt` exits with checkpoint saving

---

